{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pics/rsna.png\" alt=\"Paris\" class=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> RSNA Intracranial Hemorrhage Detection </center>\n",
    "#### <center> Prepared for Professor Miguel Alonso for CAP 5610</center>\n",
    "#### <center> Giancarlo Sanchez and Luis Caicedo </center>\n",
    "#### <center> GoogLeNET Version </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Problem\n",
    "\n",
    "Intracranial hemorrhage detection is a critical problem that requires immediate diagnosis and treatment. Traditionally, doctors have to diagnose a hemorrhage by visiaully inspecting images from angiograms and MRI scans. The symptoms for a hemmorhage could be as simple as an intense headache which may or may not actually be an intracranial hemmorhage thereby wasting time and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Data\n",
    "\n",
    "The Radiological Society of North America (RSNA) has provided collection of over 750,000 images of brain scans through Kaggle. This data comes in DICOM format so they also contain associated metadata such as patient ID and appropriate labels. The data comes with one of 6 labels: epidural, subdural, subarachnoid, intraparenchymal, intraventricular hemorrhages, and ‘any’ if any of the preceding ones are 1. A description of each type of hemorrhage follows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pics/IntraHemPic.png\" alt=\"Paris\" class=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pics/Models.png\" alt=\"Paris\" class=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim is to try to find a model that will classify the images based on the existence of a hemorrhage and then, if there is one, into each of the subtypes described above. We use png data from Kaggle because the file sizes are smaller. We want to compare the performance of 4 different pretrained models in order to achieve this task: AlexNet , ResNet , VGG 19 , and Inception. Each of these have become the industry standard in their respective years of publication for image classification. We wanted to see how well these pretrained models would classify the data. \n",
    "\n",
    "We converted the data into PNG files instead of having them as the DICOM images and then created a csv file to hold the image data along with the labels. These png files were of dimmensions 224x244. Doing this process reduced our training data from over 750k picture to about 650k pictures. Then, we restricted our dataset further to 300k pictures in Kaggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle \n",
    "\n",
    "This code was submitted using Kaggle Kernels in order to obtain a measure of the model's effectiveness. The following code will run at the following location: <center> https://www.kaggle.com/luisrex15/finalproject-alonso-gs-lct?scriptVersionId=24699703 </center>\n",
    "\n",
    "There are 10 commits found at the top left of the script. The following versions of the code correspond to the following models: \n",
    "\n",
    "<center> AlexNET: Commit Version 3 </center>\n",
    "<center> ResNET: Commit Version 4</center>\n",
    "<center> VGG11: Commit Version 6</center>\n",
    "<center> GoogLeNet: Commit Version 8</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import The Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "#Computer Vision Libraries\n",
    "import cv2\n",
    "from albumentations import Compose, ShiftScaleRotate, Resize\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "#Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/'\n",
    "train_data_images = '../input/rsna-train-stage-1-images-png-224x/stage_1_train_png_224x/'\n",
    "\n",
    "train = pd.read_csv(os.path.join(data_path,'stage_2_train.csv')) #Read file\n",
    "train[['ID','Image','Diagnosis']]=train['ID'].str.split('_',expand= True) #Split the ID column at each _\n",
    "train = train[['Image','Diagnosis','Label']] #reorder the columns\n",
    "train.drop_duplicates(inplace= True)  #drop duplicates\n",
    "train = train.pivot(index = 'Image' , columns = 'Diagnosis', values = 'Label').reset_index() #Reorganizes csv to make columns with labels instead of 6 rows for each image\n",
    "train['Image'] = 'ID_' + train['Image'] #Put ID_ back with picture ID's\n",
    "\n",
    "#Remove files that aren't of png type\n",
    "png = glob.glob(os.path.join(train_data_images, '*.png')) #list of paths to the pictures\n",
    "png = [os.path.basename(png)[:-4] for png in png] #drop the .png at the end \n",
    "png = np.array(png) #convert to a NumPy array\n",
    "\n",
    "\n",
    "train = train[train['Image'].isin(png)] # Reconcile the lists and images\n",
    "train = train[:300000] #Take the first 300k pictures\n",
    "train.to_csv('train.csv', index = False)\n",
    "print(train.shape) #just to know shape of dataset\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same code as before, just changing names\n",
    "data_path = '../input/rsna-intracranial-hemorrhage-detection/rsna-intracranial-hemorrhage-detection/'\n",
    "test_data_images = '../input/stage-2-png/'\n",
    "\n",
    "test = pd.read_csv(os.path.join(data_path,'stage_2_sample_submission.csv')) #Read file\n",
    "test[['ID','Image','Diagnosis']]=test['ID'].str.split('_',expand= True) #Split the ID column at each _\n",
    "test['Image'] = 'ID_' + test['Image'] #Put ID_ back with picture ID's\n",
    "test = test [['Image','Label']]\n",
    "test.drop_duplicates(inplace= True)  #drop duplicates\n",
    "test.to_csv('test.csv', index = False)\n",
    "print(test.shape) #just to know shape of dataset \n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Data Ready For Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNA(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, path, labels, transform=None):       \n",
    "        self.path = path\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):        \n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.path, self.data.loc[idx, 'Image'] + '.png')\n",
    "        img = cv2.imread(img_name)   \n",
    "        \n",
    "        if self.transform:                   \n",
    "            augmented = self.transform(image=img)\n",
    "            img = augmented['image']   \n",
    "            \n",
    "        if self.labels:            \n",
    "            labels = torch.tensor(\n",
    "                self.data.loc[idx, ['epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural', 'any']])\n",
    "            return {'image': img, 'labels': labels}    \n",
    "        \n",
    "        else:                  \n",
    "            return {'image': img}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = Compose([ShiftScaleRotate(),ToTensor()])\n",
    "transform_test = Compose([ToTensor()])\n",
    "\n",
    "train_dataset = RSNA(csv_file='train.csv', path=train_data_images, transform=transform_train, labels=True)\n",
    "test_dataset = RSNA(csv_file='test.csv', path=test_data_images, transform=transform_test, labels=False)\n",
    "\n",
    "batch_size = 64\n",
    "data_train_generator = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
    "data_test_generator = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model and Download Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model0 = models.googlenet(pretrained = True)\n",
    "model = torch.nn.Sequential(model0, torch.nn.Linear(1000, 6)).to(device)\n",
    "\n",
    "\n",
    "#model = {alexnet,vgg, resnet18}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "optimizer = optim.Adam(model.parameters(), lr=4e-5)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Train Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,criterion,optimizer,num_epochs=1):\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        model.train()    \n",
    "        tr_loss = 0\n",
    "\n",
    "        tk0 = tqdm(data_train_generator, desc=\"Iteration\")\n",
    "\n",
    "        for step, batch in enumerate(tk0):\n",
    "\n",
    "            inputs = batch[\"image\"]\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            inputs = inputs.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        torch.save(model.state_dict(), f'resnext50_{epoch}.pth') \n",
    "\n",
    "        epoch_loss = tr_loss / len(data_train_generator)\n",
    "        print('Training Loss: {:.4f}'.format(epoch_loss))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trained = train_model(model,criterion,optimizer,num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_pred = np.zeros((len(test_dataset) * 6, 1))\n",
    "\n",
    "for i, batch_ in enumerate(tqdm(data_test_generator)):\n",
    "    batch_ = batch_[\"image\"]\n",
    "    batch_ = batch_.to(device, dtype=torch.float)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        pred = model(batch_)\n",
    "        \n",
    "        test_pred[(i * batch_size * 6):((i + 1) * batch_size * 6)] = torch.sigmoid(\n",
    "            pred).detach().cpu().reshape((len(batch_) * 6, 1))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission To Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission =  pd.read_csv(os.path.join(data_path, 'stage_2_sample_submission.csv'))\n",
    "submission = pd.concat([submission.drop(columns=['Label']), pd.DataFrame(test_pred)], axis=1)\n",
    "submission.columns = ['ID', 'Label']\n",
    "submission.to_csv('submission-1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Public Score | Private Score |\n",
    "|-------|--------------|---------------|\n",
    "| AlexNET| 0.88097 |  0.27346                  |\n",
    "| ResNET| 0.53291 |   0.20561                 |\n",
    "| VGG| 0.12557| 0.33155                 |\n",
    "| GoogLeNET  | 0.73971|    0.25604                |\n",
    "\n",
    "After training on the 300,000 images, we find that VGG is the best model out of the 4 when we look at the Public Score. AlexNET and GoogLeNET performed similarly. On prelimiary runs, we ran these four models on 100 images and plotted the training errors for each of 15 epochs: \n",
    "\n",
    "<img src=\"Pics/EpochsErrors.png\" alt=\"Paris\" class=\"center\">\n",
    "\n",
    "Here we note that VGG converges very rapidly to a low loss error while AlexNET ang GoogLeNET converge to almost the same loss metric. This observation is further confirmed when we look at their similar Public Scores on Kaggle. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
